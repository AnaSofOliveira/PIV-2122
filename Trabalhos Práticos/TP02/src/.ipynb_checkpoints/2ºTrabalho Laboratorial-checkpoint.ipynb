{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d5731fa-f8a2-4ee8-98ad-6eca6eb53bc8",
   "metadata": {
    "tags": []
   },
   "source": [
    "<center> <img src=\"imgs/relatório/isel.png\" width=150 height=150> </center>\n",
    "<center> <h2> Instituto Superior de Engenharia de Lisboa </h2> </center>\n",
    "<center> <b>Ano Lectivo:</b> 2021/2022 </center>\n",
    "</br>\n",
    "<center> <h2> Processamento de Imagem e Visão </h2> </center>\n",
    "<center> <h3> 2º Trabalho Laboratorial - Deteção de movimento para Tracking de objetos </h3> </center>\n",
    "</br>\n",
    "<center> <img src=\"imgs/relatório/classificacao_frame_1443.jpg\" width=512 height=512> </center>\n",
    "\n",
    "\n",
    "\n",
    "<center><h3> Docentes: </h3></center>\n",
    "<center><h5>Pedro Mendes Jorge e João Costa</h5></center>\n",
    "</br>\n",
    "<center>    \n",
    "    <h3>Desenvolvido por:</h3> \n",
    "    <table style=\"width: 50%\">\n",
    "        <tr>\n",
    "          <th><center>Aluno</center></th>\n",
    "          <th><center>Número</center></th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td><center>Ana Sofia Oliveira</center></td>\n",
    "          <td><center>39275</center></td>\n",
    "        </tr> \n",
    "    </table>\n",
    "</center>\n",
    "</br>\n",
    "</br>\n",
    "<hr class=\"solid\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39a1fb6-c248-4b01-ae3d-f00225a6e379",
   "metadata": {},
   "source": [
    "<center> <h2> Índice </h2> </center>\n",
    "\n",
    "* [Introdução](#intro)\n",
    "* [Desenvolvimento](#desenvolvimento)\n",
    "    * [1. Leitura de imagens](#leitura)\n",
    "    * [2. Conversão para níveis de cinzento](#niveisCinzento)\n",
    "    * [3. Binarização](#binarizacao)\n",
    "    * [4. Melhoramento da imagem](#melhoramento)\n",
    "    * [5. Extração de componentes conexos](#extracaoComponentes)\n",
    "    * [6. Extração de propriedades](#extracaoPropriedades)\n",
    "    * [7. Classificação de objectos](#classificacao)\n",
    "* [Bateria de Testes](#bateriaTestes)\n",
    "* [Conclusão](#conclusao)\n",
    "\n",
    "\n",
    "<hr class=\"solid\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6257341c-893f-41c8-9564-cc89d8abbd59",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "<center> <h2 id='intro'> Introdução </h2> </center>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Este trabalho prático irá ser desenvolvido no ambito da unidade curricular de Processamento de Imagem e Visão e tem como principal objectivo construir um sistema capaz de identificar pessoas e carros em movimento num vídeo. Para isto irá ser necessário identificar as zonas em que existe movimento e distinguir pessoas e carros dos restantes objectos que possam ser apresentados no vídeo.\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Irei apresentar um sistema capaz de implementar diversas técnicas de processamento de imagem, lecionadas no decorrer da unidade curricular, tais como subtração de fundo, conversão de imagens para níveis de cinzento, binarização de imagens, melhoramento de imagens, extração de componentes conexos, extração de propriedades e classificação de objectos. Todos estes conceitos irão ser abordados no decorrer deste documento.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Serão apresentados testes e resultados por cada técnica de processamento em foco, bem como os testes e resultados finais. Todos estes resultados serão apresentados como imagens, excepto na extração de propriedades em que, para além das imagens, será complementado com um ficheiro .txt com as propriedades de todos os objectos analisados na imagem corrente. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Durante a implementação deste trabalho serão utilizados métodos de processamento de imagem, disponibilizados pela biblioteca Open CV, métodos de visualização de dados e de apresentação gráfica, disponibilizados pela biblioteca Matplotlib.pyplot, e métodos para cálculo matemático, disponibilizados pela biblioteca Numpy.</p>\n",
    "\n",
    "<hr class=\"solid\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67454ebe-b64e-4c6c-847b-ed3649007fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "path = \"video/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330d368c-f3f3-4252-9a3d-831dcc24a90e",
   "metadata": {},
   "source": [
    "<hr class=\"solid\">\n",
    "<a id='desenvolvimento'></a>\n",
    "<center> <h2> Desenvolvimento </h2> </center>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Um sistema de deteção de movimento é um sistema capaz de detetar os objectos em movimento e classificá-los de acordor com a sua categoria. Este sistema verifica uma determinado vídeo analizando-o frame-by-frame e classificandos os objectos em cada uma delas. No meu caso pertendia construir um detetor de movimento capaz de distiguir carros, pessoas e outros objectos em movimento. Para cada frame, pretendia obter frames de saída com todos os objectos identificados classificados. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp;Desta forma, podemos decrever o seguinte diagrama geral do sistema a implementar:</p>\n",
    "<center> <img src=\"imgs/relatório/detetor movimento.png\" width=512 height=512> </center>\n",
    "</br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp;O sistema deve contemplar os seguintes processos:</p>\n",
    "<ol>\n",
    "    <li>Leitura frame-by-frame do vídeo;</li>\n",
    "    <li>Estimação de fundo;</li>\n",
    "    <li>Deteção de pixeis ativos;</li>\n",
    "    <li>Melhoramento da imagem;</li>\n",
    "    <li>Extração de propriedades dos objectos da imagem;</li>\n",
    "    <li>Classificação dos objectos da imagem.</li>\n",
    "    <li>[Adicional] Segmentação (tracking) dos objectos entre frames.</li>\n",
    "</ol>\n",
    "</br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp;Cada uma destas técnicas será descrita como um bloco no sistema implementado. No decorrer deste documento irei detalhar todas as técnicas acima indicadas. Podemos agora detalhar o sistema conforme apresentado no diagrama abaixo. </p>\n",
    "<center> <img src=\"imgs/relatório/sistema geral.png\" width=512> </center>\n",
    "</br>\n",
    "<a id='estimacao'></a>\n",
    "<h3> 1. Estimação de Fundo </h3>\n",
    "<center> <img src=\"imgs/relatório/estimacao de fundo.png\" width=512> </center>\n",
    "\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A estimação de fundo é um técnica utilizada em sistemas de vídeo para distinguir os objectos em movimento daqueles que se mantém estáticos. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; O fundo de um vídeo, embora possa ser estático, pode ser influenciado por diversos fatores externos, como a alteração da luminosidade ao longo do tempo. Isto faz com que, quando mantemos sempre o mesmo fundo sem poderar estes possíveis fatores, em algumas situações, possamos estar a identificar partes do fundo como movimento, indevidamente. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Assim, para colmatar estes fatores, o fundo depende em parte da última frame lida e torna-se, também ele, dinâmico ao longo do tempo. Para isto, define-se que caso exita movimento na última frame lida o novo fundo é igual ao fundo anterior. No entanto, nas zonas em que não existe movimento na última frame lida, o fundo é composto por parte da frame lida e parte do antigo fundo, na mesma zona. O fator que define se queremos manter mais informação mais recente - maior preponderancia da última frame lida - ou mais antiga - maior preponderancia do fundo, é o fator $\\alpha$ definido entre [0, 1].</p> \n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A fórmula geral da estimação de fundo é a seguinte: </p>\n",
    "<center><b>Pixéis de movimento:</b> </center>\n",
    "<center> \n",
    "$|I_{n}(r,c) + B_{n}(r,c)| > T_{n}(r,c)$\n",
    "</center>\n",
    "</br>\n",
    "<center><b>Cálculo do fundo:</b> </center>\n",
    "<center>\n",
    "$B_{n+1}(r,c) = \\begin{cases} B_{n}(r,c), & \\mbox{se } \\mbox{ (r,c) pixel movimento} \\\\ \\alpha B_{n}(r,c) + (1 - \\alpha) I_{n}(r,c), & \\mbox{se } \\mbox{ caso contrario} \\end{cases}$\n",
    "</center>\n",
    "</br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Para detetar os pixeis de movimento construí a função deteta_movimento e para calcular o fundo construí a função estima_fundo. A função deteta_movimento recorre à função absdiff para calcular a diferença entre o fundo e a frame corrente, e guarda as posições onde essa diferença é superior ao threshlod definido. A função estima_fundo recorre à função addWeighted para calcular o novo fundo.</p>\n",
    "\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; O threshold escolhido para a deteção do movimento foi definido após alguns testes. Caso o threshold fosse muito elevado existam objectos de interesse cujo movimento não era identificado devido à baixa variação de cor entre o fundo e a frame de análise. Caso o threshold fosse muito baixo, qualquer variação era identificada como pixel de movimento. Assim, este threshold foi escolhido com base nos valores de teste que melhor serviam o propósito do trabalho. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; O valor de $\\alpha$ foi escolhido de forma a que o fundo mantivesse as mesmas caracteristicas e fosse pouco influenciado pela frame corrente. Caso contrário qualquer movimento entre o fundo e a frame iria durar mais tempo até que fosse atenuado na imagem, fazendo com que o peso das frames anteriores no fundo fosse maior. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf7801d3-9422-4be6-a3d6-9e6630de7e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deteta_movimento(fundo, imagem_seguinte, threshold): \n",
    "\n",
    "    diff = cv2.absdiff(fundo, imagem_seguinte) \n",
    "    movimento = diff > threshold\n",
    "\n",
    "    return movimento\n",
    "\n",
    "\n",
    "def estima_fundo(fundo, imagem_seguinte, movimento, alfa = 0.9):\n",
    "    \n",
    "    pixeis_fundo = fundo*movimento\n",
    "    pixeis_movimento = cv2.addWeighted(fundo, alfa, imagem_seguinte, (1-alfa), 0)*np.invert(movimento)\n",
    "    \n",
    "    fundo = cv2.add(pixeis_fundo, pixeis_movimento)\n",
    "    \n",
    "    return fundo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "460f5b2f-c3d0-4475-b349-c8980b0fdc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(path + \"camera1.mov\")\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "_, background = capture.read()\n",
    "_, frame = capture.read()\n",
    "frameCount = 1\n",
    "\n",
    "while capture.isOpened():\n",
    "\n",
    "    _, next_frame = capture.read() \n",
    "    frameCount = frameCount + 1\n",
    "\n",
    "    # Deteção de movimento e estimação do fundo \n",
    "    movimento = deteta_movimento(frame, next_frame, 20)\n",
    "    background = estima_fundo(background, next_frame, movimento, 0.9)\n",
    "\n",
    "    cv2.imshow(\"Fundo\", background)\n",
    "\n",
    "    frame = next_frame\n",
    "    key = cv2.waitKey(int(1000/fps))\n",
    "\n",
    "    if(key == 27 or frameCount == total_frames-1): \n",
    "        cv2.destroyAllWindows()\n",
    "        capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bea6e78-2579-4f16-b921-8d58193d4883",
   "metadata": {},
   "source": [
    "<a id='pixeis_ativos'></a>\n",
    "<h3> 2. Deteção de pixeis ativos </h3>\n",
    "<center> <img src=\"imgs/relatório/Deteção de pixeis ativos.png\" width=512></center>\n",
    "<br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A deteção de pixeis ativos é essencial neste sistema porque é o primeiro passo para nos peritir identificar regiões onde existiu movimento e, mais tarde, classificar essas regiões. </p> \n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; No bloco anterior já tinham sido calculados os pixeis onde existiu movimento, que correspondem aos pixéis ativos na imagem, pelo que, de forma a conseguirmos apresentar graficamente esta informação criei a função obtem_pixeis_ativos, que multiplica todos os pixeis da matriz de movimento por 255. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; <b>APRESENTAR RESULTADO EXEMPLO</b> </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "988aeb36-b3b9-4ce3-acf9-f2c3970ee88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtem_pixeis_ativos(imagem_movimento):\n",
    "    pixeis_ativos = imagem_movimento.astype(np.uint8)*255\n",
    "    \n",
    "    return pixeis_ativos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "253c8302-f158-4573-a3ae-dc514cc438ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(path + \"camera1.mov\")\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "_, background = capture.read()\n",
    "_, frame = capture.read()\n",
    "frameCount = 1\n",
    "\n",
    "while capture.isOpened():\n",
    "\n",
    "    _, next_frame = capture.read() \n",
    "    frameCount = frameCount + 1\n",
    "\n",
    "    # Deteção de movimento e estimação do fundo \n",
    "    movimento = deteta_movimento(frame, next_frame, 20)\n",
    "    background = estima_fundo(background, next_frame, movimento, 0.9)\n",
    "\n",
    "    # Pixeis ativos\n",
    "    pixeis_ativos = obtem_pixeis_ativos(movimento)\n",
    "    \n",
    "    cv2.imshow(\"Pixeis Ativos\", pixeis_ativos)\n",
    "\n",
    "    frame = next_frame\n",
    "    key = cv2.waitKey(int(1000/fps))\n",
    "\n",
    "    if(key == 27 or frameCount == total_frames-1): \n",
    "        cv2.destroyAllWindows()\n",
    "        capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa61387-7382-450c-b425-c63abe42fc09",
   "metadata": {},
   "source": [
    "<a id='melhoramento'></a>\n",
    "<h3> 3. Melhoramento de Imagem </h3>\n",
    "<center> <img src=\"imgs/relatório/Melhoramento de imagem.png\" width=512></center>\n",
    "<br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Com a identificação dos pixeis ativos existiam alguns pixeis que, para o propósito do trabalho não interssavam ser considerados e algumas regiões que, embora estivessem separadas, pertenciam ao mesmo objecto. Assim, foi necessário usar técnicas de melhoramento de imagem de forma a definir as áreas de interesse. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Como a frame de pixeis ativos continha 3 canais de cor e de forma a facilitar o processamento da imagem foi realizada a binarização da mesma antes de prosseguir com o melhoramento da imagem. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Conforme indicado anteriormente, o objectivo do trabalho é a identificação de pessoas e carros. Estes objectos, à primeira vista, no vídeo de análise têm formas geométricas gerais muito identicas pelo que o elemento estruturante usado para o melhoramento da imagem foi o cv2.MORPH_RECT. Dependendo da operação morfológica usada, este elemento irá alterar o seu tamanho. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Após a escolha do elemento estruturante - retangulo - foi necessário escolher os operadores morfológicos a usar e qual a sua ordem. Assim, a sequência aplicada no âmbito do trabalho foi a seguinte: </p>\n",
    "\n",
    "<h4>1. Fecho</h4>\n",
    "<ul>\n",
    "    <li>Tamanho do elemento estruturante: (3,3)</li>\n",
    "    <li>Objectivo: Remover pixeis activos cujo com tamanho igual ao inferior ao tamanho do elemento estruturante para que fosse considerados para a extração de propriedades. </li>\n",
    "    \n",
    "</ul>\n",
    "\n",
    "<h4>2. Dilatação</h4>\n",
    "<ul>\n",
    "    <li>Tamanho do elemento estruturante: (8,8)</li>\n",
    "    <li>Objectivo: Aumentar as zonas onde foi identificado movimento para facilitar a identificação dos objectos. </li>\n",
    "</ul>\n",
    "\n",
    "<h4>3. Abertura</h4>\n",
    "<ul>\n",
    "    <li>Tamanho do elemento estruturante: (15,10)</li>\n",
    "    <li>Objectivo: Juntar áreas que após a dilatação ainda pudesses estar separadas mas que correspondiam ao mesmo objecto.</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; O tamanho dos elementos estruturantes aplicados são os que, após alguns testes, melhor cumpriram o objectivo do trabalho. </p>\n",
    "\n",
    "\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; <b>APRESENTAR RESULTADO EXEMPLO</b> </p>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "891ec74c-66c1-4bfb-9233-132eb186e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizaImagem(pixeis_ativos): \n",
    "    kernel_median = (5,5)\n",
    "    next_frame_g = cv2.cvtColor(pixeis_ativos, cv2.COLOR_BGR2GRAY)\n",
    "    next_frame_g = cv2.blur(next_frame_g, kernel_median)\n",
    "    th, binary = cv2.threshold(next_frame_g, 75, 255, cv2.THRESH_BINARY) \n",
    "    \n",
    "    return th, binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6d88b-74e8-498f-b457-5adf7aa9b139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def melhoraImagem(pixeis_ativos): \n",
    "    \n",
    "    _, binary = binarizaImagem(pixeis_ativos)\n",
    "    \n",
    "    elementoEstruturante = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))\n",
    "    fecho = cv2.morphologyEx(binary, cv2.MORPH_CLOSE, elementoEstruturante) \n",
    "\n",
    "    elementoEstruturante = cv2.getStructuringElement(cv2.MORPH_RECT, (8,8))\n",
    "    dilatacao = cv2.dilate(fecho, elementoEstruturante, iterations = 3)\n",
    "\n",
    "    elementoEstruturante = cv2.getStructuringElement(cv2.MORPH_RECT, (14,7))\n",
    "    abertura = cv2.morphologyEx(dilatacao, cv2.MORPH_OPEN, elementoEstruturante)\n",
    "\n",
    "    return abertura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a4f7514-b801-428f-954c-2c10df60d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "capture = cv2.VideoCapture(path + \"camera1.mov\")\n",
    "fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "total_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "_, background = capture.read()\n",
    "_, frame = capture.read()\n",
    "frameCount = 1\n",
    "\n",
    "while capture.isOpened():\n",
    "\n",
    "    _, next_frame = capture.read() \n",
    "    frameCount = frameCount + 1\n",
    "\n",
    "    # Deteção de movimento e estimação do fundo \n",
    "    movimento = deteta_movimento(frame, next_frame, 20)\n",
    "    background = estima_fundo(background, next_frame, movimento, 0.9)\n",
    "\n",
    "    # Pixeis ativos\n",
    "    pixeis_ativos = obtem_pixeis_ativos(movimento)\n",
    "    \n",
    "    # Melhoramento da imagem\n",
    "    melhorada = melhoraImagem(pixeis_ativos)\n",
    "    cv2.imshow(\"Pixeis Ativos - Melhoramento de Imagem\", melhorada)\n",
    "    \n",
    "    frame = next_frame\n",
    "    key = cv2.waitKey(int(1000/fps))\n",
    "\n",
    "    if(key == 27 or frameCount == total_frames-1): \n",
    "        cv2.destroyAllWindows()\n",
    "        capture.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a5761-288f-47ee-8937-740eb95a450b",
   "metadata": {},
   "source": [
    "<a id='extracao'></a>\n",
    "<h3> 4. Extração de Propriedades </h3>\n",
    "<center><img src=\"imgs/relatório/Extração Propriedades.png\" width=512px></center>\n",
    "<br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Após o melhoramento da imagem, é possível extraír caracteristicas ou propriedades das regiões ativas. Assim, foi efetuada a análise da área, largura e altura destas regiões e verificadas quais as caracteristicas que melhor diferenciavam os objectos a classificar. Para isso, primeiro foi guardada a informação das caracteristicas extraídas de várias imagens num ficheiro .txt que para cada imagem identificava o objecto e as suas caracteristicas. Após esta extração, foram mantidas apenas as imagens em que as caracteristicas dos objectos minimizavam as caracteriticas iniciais de deteção. Por exemplo, se quisermos saber a partir de que área devemos considerar o objecto para classificação e, no vídeo, existem frames onde o objecto tem menores porporções, então precisamos de considerar a frame onde o objecto tem menor tamanho para extraír o menor valor da área que o objecto pode ter. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Desta forma foram analisadas as seguintes frames e as propriedades dos objectos nela identificados: </p>\n",
    "\n",
    "<center><img src=\"imgs/propriedades/frame_1445.jpg\" width=500px></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Frame</th>\n",
    "        <th>Objecto</th>\n",
    "        <th>Área</th>\n",
    "        <th>Largura</th>\n",
    "        <th>Altura</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1445</td>\n",
    "        <td>1 - Pessoa</td>\n",
    "        <td>3618.5</td>\n",
    "        <td>53</td>\n",
    "        <td>87</td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td>1445</td>\n",
    "        <td>2 - Pessoa</td>\n",
    "        <td>2024.5</td>\n",
    "        <td>43</td>\n",
    "        <td>57</td>\n",
    "    <tr>\n",
    "    <tr>\n",
    "        <td>1445</td>\n",
    "        <td>3 - Carro</td>\n",
    "        <td>4402.0</td>\n",
    "        <td>92</td>\n",
    "        <td>57</td>\n",
    "    <tr>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/propriedades/frame_1860.jpg\" width=500px></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Frame</th>\n",
    "        <th>Objecto</th>\n",
    "        <th>Área</th>\n",
    "        <th>Largura</th>\n",
    "        <th>Altura</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>1860</td>\n",
    "        <td>1 - Pessoa</td>\n",
    "        <td>1797.0</td>\n",
    "        <td>41</td>\n",
    "        <td>54</td>\n",
    "    <tr>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/propriedades/frame_2342.jpg\" width=500px></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Frame</th>\n",
    "        <th>Objecto</th>\n",
    "        <th>Área</th>\n",
    "        <th>Largura</th>\n",
    "        <th>Altura</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2342</td>\n",
    "        <td>1 - Pessoa</td>\n",
    "        <td>2138.0</td>\n",
    "        <td>42</td>\n",
    "        <td>62</td>\n",
    "    <tr>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "<center><img src=\"imgs/propriedades/frame_2931.jpg\" width=500px></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Frame</th>\n",
    "        <th>Objecto</th>\n",
    "        <th>Área</th>\n",
    "        <th>Largura</th>\n",
    "        <th>Altura</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2931</td>\n",
    "        <td>1 - Pessoa</td>\n",
    "        <td>1998.5</td>\n",
    "        <td>49</td>\n",
    "        <td>49</td>\n",
    "    <tr>\n",
    "</table>\n",
    "<br>\n",
    "\n",
    "\n",
    "<center><img src=\"imgs/propriedades/frame_2949.jpg\" width=500px></center>\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>Frame</th>\n",
    "        <th>Objecto</th>\n",
    "        <th>Área</th>\n",
    "        <th>Largura</th>\n",
    "        <th>Altura</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>2949</td>\n",
    "        <td>1 - Carro</td>\n",
    "        <td>3984.5</td>\n",
    "        <td>81</td>\n",
    "        <td>57</td>\n",
    "    <tr>\n",
    "</table>\n",
    "<br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A implementação da extração de propriedades foi realizada no método extracaoPropriedades e irá ser detalhado com maior pormenor no próximo tópico.  </p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85a52c27-42c1-4f4e-8dd2-40bc6c513a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtemArea(contorno): \n",
    "    return round(cv2.contourArea(contorno), 2)\n",
    "\n",
    "def obtemCentroide(contorno):\n",
    "    momentos = cv2.moments(contorno)\n",
    "    centro_x = round(momentos['m10']/momentos['m00'])\n",
    "    centro_y = round(momentos['m01']/momentos['m00'])\n",
    "    \n",
    "    return (centro_x, centro_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca050e49-84d1-4905-9d0c-af26aa9cd90b",
   "metadata": {},
   "source": [
    "<a id='extracaoComponentes'></a>\n",
    "<h3> 5. Classificação </h3>\n",
    "<center><img src=\"imgs/relatório/diagrama_extracao_elementos_conexos.png\"></center>\n",
    "<br>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; É nesta fase que o processo de classificação própriamente dito inicia, sendo identificadas as caracteristicas (features) de cada objecto a classificar pelo sistema. Após a análise das propriedades nas frames anteriormente apresentadas foram retiradas as seguinte ilações: </p>\n",
    "<ul>\n",
    "    <li>Área de pessoas compreendida entre [1500, 3500] pixéis;</li>\n",
    "    <li>Área de carros superior a 3500 pixéis;</li>\n",
    "    <li>Carros têm maior área que as Pessoas;</li>\n",
    "    <li>Largura de pessoas superior a 40 pixéis e Altura de pessoas superior a 50 pixéis;</li>\n",
    "    <li>Largura de carros superior a 81 pixéis pixéis e Altura de carros superior a 57 pixéis;</li>\n",
    "    <li>Pessoas têm Altura > Largura; </li>\n",
    "    <li>Carros têm Largura > Altura; </li>\n",
    "    <li>Objectos a classificar têm de Largura entre [Largura mínima Pessoa; Largura Máxima Carro] e Altura entre [Altura mínima Carro; Altura máxima Pessoa];</li>\n",
    "    <li>Objectos a classificar têm área superior à área mínima da Pessoa (1500 pixéis).</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Através das ilações apresentadas acima, complementei o método extraçãoPropriedades para que apenas verificasse as propriedades dos objectos com largura, altura e área superiores aos valores apresentados acima, e desenvolvi o método classificaObjecto para que classificasse os objectos de acordo com as diferenças aprensentadas acima. </p>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c55d656-bd4a-451f-9f2f-8178075261c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificaObjecto(area, largura, altura): \n",
    "    if (altura>=largura) and (area > 1500):\n",
    "        nome = 'Pessoa'\n",
    "        cor = (0, 0, 255)  \n",
    "\n",
    "    elif (area >= 3500):\n",
    "        nome = 'Carro'\n",
    "        cor = (0, 255, 0)\n",
    "\n",
    "    else:\n",
    "        nome = 'Objecto'\n",
    "        cor = (255, 0, 0)\n",
    "        \n",
    "    return nome, cor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "66c0c110-38e6-44e1-9c41-1db3cc40d118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extracaoPropriedades(contornos, frameCount, objectos_encontrados, ID = 1):\n",
    "    for cnt in contornos:         \n",
    "        # Calculate area and remove small elements\n",
    "        area = obtemArea(cnt)\n",
    "        posx, posy, largura, altura = cv2.boundingRect(cnt)\n",
    "        \n",
    "        if ((largura,altura) >= (40,50) and (area > 1500)):\n",
    "            \n",
    "            centroide = obtemCentroide(cnt)\n",
    "            nome, cor = classificaObjecto(area, largura, altura)\n",
    "            \n",
    "            propriedades = [ID, {\"nome\": nome, \n",
    "                            \"area\": area, \n",
    "                            \"centroide\": centroide, \n",
    "                            \"posx\": posx, \n",
    "                            \"posy\": posy, \n",
    "                            \"largura\": largura,\n",
    "                            \"altura\": altura, \n",
    "                            \"cor\": cor}, \n",
    "                           frameCount]\n",
    "            \n",
    "            ID = ID + 1\n",
    "            objectos_encontrados.append(propriedades)\n",
    "            \n",
    "    return objectos_encontrados, ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16d4630-9bfa-4a20-8b32-caf43c49bb44",
   "metadata": {},
   "source": [
    "<a id='tracking'></a>\n",
    "<h3> 6. Tracking </h3>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Após a classificação dos objectos por frame é necessário saber quais os objectos que já foram identificados em frames anteriores e se ainda se mantém no plano de imagem. Estes objectos deverão ser atualizados porém não devem ser considerados como novos objectos nos sistema. </p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Este processo foi implementado no método trackObjects que compara os objectos detetados em frames anteriores com os objectos detetados na frame corrente. Caso o objecto já tenha sido detetado mantém a referência do ID previamente detetado, atualizando apenas as suas propriedades e a referência para a última frame onde foi observado. Caso o objecto não tenha sido detetado em frames anteriores adiciona-o à lista de objectos em tracking. Cada objecto mantém-se em tracking até que deixe de ser identificado em 5 frames seguidas. Caso isto aconteça, o objecto deixa de ser \"acompanhado\". </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f64f5551-cb48-4c4f-8851-46471ac7a4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def updateObjectOnTrack(old_object, new_object):\n",
    "    # ID antigo, propriedades novas e frameID novo\n",
    "    updated_object = [old_object[0], new_object[1], new_object[2]]\n",
    "    \n",
    "    return updated_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd7fc1f2-b81a-4e57-9561-07aaa47fb161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackObjects(tracking_objects, frame_objects, frameAtual): \n",
    "    segment_objects = tracking_objects.copy()    \n",
    "    for frame_object in frame_objects:\n",
    "        object_exists = False\n",
    "        for track_object in tracking_objects:\n",
    "            obj_centroid = frame_object[1][\"centroide\"]\n",
    "            trk_centroid = track_object[1][\"centroide\"]\n",
    "            \n",
    "            distancia = math.hypot(trk_centroid[0] - obj_centroid[0], trk_centroid[1] - obj_centroid[1])\n",
    "            \n",
    "            if(distancia < 10):\n",
    "                # object in track\n",
    "                object_exists = True\n",
    "                track_object = updateObjectOnTrack(track_object, frame_object)\n",
    "                continue\n",
    "            \n",
    "        if(not object_exists):\n",
    "            # new object on track\n",
    "            segment_objects.append(frame_object)\n",
    "             \n",
    "    tracking_objects = []\n",
    "    # Valida se os objectos em track foram identificados nas últimas 5 frames\n",
    "    for objecto in segment_objects:\n",
    "        last_frame_seen = objecto[2]\n",
    "        if(frameAtual - last_frame_seen < 5):\n",
    "            tracking_objects.append(objecto)\n",
    "    \n",
    "    return tracking_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2156d22b-05e0-4e2c-9b65-3950619b64a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def desenhaObjectos(tracking_objects, frame):\n",
    "    contoured_frame = frame.copy()\n",
    "    for objecto in tracking_objects: \n",
    "        cv2.rectangle(contoured_frame, (objecto[1][\"posx\"], objecto[1][\"posy\"]), (objecto[1][\"posx\"] + objecto[1][\"largura\"], objecto[1][\"posy\"] + objecto[1][\"altura\"]), objecto[1][\"cor\"], 2)\n",
    "        cv2.putText(contoured_frame, str(objecto[0]) + \" \" + str(objecto[1][\"nome\"]), (objecto[1][\"posx\"], objecto[1][\"posy\"] - 7), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), thickness=1)\n",
    "    \n",
    "    return contoured_frame "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f61c62-40e4-460e-974d-3389992d8097",
   "metadata": {},
   "source": [
    "<hr class=\"solid\">\n",
    "<a id='bateriaTestes'></a>\n",
    "<h3> Bateria de Testes Finais </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aac5ce9-de18-476c-a6e3-d75c29483367",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MotionDetection(inVideo):\n",
    "    \n",
    "    capture = cv2.VideoCapture(inVideo)\n",
    "    \n",
    "    fps = capture.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = capture.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    print(\"FPS:\", fps)\n",
    "    print(\"Total Frames:\", total_frames)\n",
    "\n",
    "    _, background = capture.read()\n",
    "    _, frame = capture.read()\n",
    "    \n",
    "    frameCount = 1\n",
    "    tracking_objects = []\n",
    "\n",
    "    while capture.isOpened():\n",
    "\n",
    "        _, next_frame = capture.read() \n",
    "        frameCount = frameCount + 1\n",
    "\n",
    "        # Deteção de movimento e estimação do fundo \n",
    "        movimento = deteta_movimento(frame, next_frame, 20)\n",
    "        background = estima_fundo(background, next_frame, movimento, 0.9)\n",
    "#         cv2.imshow(\"background\", background)\n",
    "#         cv2.imshow(\"Next Frame\", next_frame)\n",
    "\n",
    "        # Pixeis ativos\n",
    "        pixeis_ativos = obtem_pixeis_ativos(movimento)\n",
    "#         cv2.imshow(\"Pixeis ativos\", pixeis_ativos)\n",
    "\n",
    "        # Melhoramento da imagem\n",
    "        melhorada = melhoraImagem(pixeis_ativos)\n",
    "#         cv2.imshow(\"Melhorada\", melhorada)\n",
    "\n",
    "        # Extração de regiões\n",
    "        contoured = next_frame.copy()\n",
    "        contours, hierarquia = cv2.findContours(melhorada, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Extração de propriedades e classificação de objectos\n",
    "        frame_objects = []\n",
    "        frame_objects, ID = extracaoPropriedades(contours, frameCount, frame_objects)\n",
    "        \n",
    "        # Tracking objects\n",
    "        tracking_objects = trackObjects(tracking_objects, frame_objects, frameCount) \n",
    "        \n",
    "        # Desenha objectos\n",
    "        contoured_frame = desenhaObjectos(tracking_objects, next_frame)\n",
    "        cv2.imshow(\"Contoured\", contoured_frame) \n",
    "\n",
    "#         if(frameCount == 2949 or frameCount == 2931 or frameCount == 2342 or frameCount == 1860 or frameCount == 1445):\n",
    "#             imagem_numerada = next_frame.copy()\n",
    "#             cv2.imwrite(\"imgs/pixeis_ativos/frame_\" + str(frameCount) + \".jpg\", pixeis_ativos)\n",
    "#             cv2.imwrite(\"imgs/melhoramento/frame_\" + str(frameCount) + \".jpg\", melhorada)\n",
    "\n",
    "#             file_name = \"imgs/propriedades/frame_\" + str(frameCount) + \".txt\"\n",
    "#             file = open(file_name, \"w\")\n",
    "#             for objecto in frame_objects:\n",
    "#                 for prop in objecto[1]:\n",
    "#                     imagem_numerada = cv2.putText(imagem_numerada, str(objecto[0]), objecto[1][\"centroide\"], cv2.FONT_ITALIC, 1, (0,255,0), thickness=2)\n",
    "#                     file.write(\"\\n\" + prop + \": \" + str(objecto[1][prop]))\n",
    "#                 file.write(\"\\n\")\n",
    "#             file.close()\n",
    "#             cv2.imwrite(\"imgs/propriedades/frame_\" + str(frameCount) + \".jpg\", imagem_numerada)\n",
    "#             cv2.imwrite(\"imgs/classificacao/frame_\" + str(frameCount) + \".jpg\", contoured_frame)\n",
    "\n",
    "        frame = next_frame\n",
    "\n",
    "        key = cv2.waitKey(int(1000/fps))\n",
    "\n",
    "        if(key == 27 or frameCount == total_frames-1): \n",
    "            cv2.destroyAllWindows()\n",
    "            capture.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd1ab7c-9855-4456-a870-dd51af0fd649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FPS: 25.0\n",
      "Total Frames: 3065.0\n"
     ]
    }
   ],
   "source": [
    "path = \"video/\"\n",
    "inVideo = \"camera1.mov\"\n",
    "\n",
    "MotionDetection(path + inVideo)\n",
    "print(\"ACABOU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5ffbe0-4de4-4992-a07e-536a9b6c694c",
   "metadata": {},
   "source": [
    "<hr class=\"solid\">\n",
    "<a id='conclusao'></a>\n",
    "<center> <h2 id='conclusao'> Conclusão </h2> </center>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Com este trabalho foi possível implementar um detetor de movimento com tracking de objectos baseado nos conhecimentos adquiridos na unidade curricular de Processamento de Imagem e Visão.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; No decorrer do projeto foi possível obter e analisar propriedades necessárias para classificar a maioria dos objetos apresentados no vídeo fornecido, usando técnicas como subtração de fundo, conversão de imagens para níveis de cinzento, binarização de imagens, melhoramento de imagens, extração de componentes conexos, extração de propriedades e classificação de objectos.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A subtração de fundo permite-nos extrair a informação dos pixeis em movimento e atualizar o fundo de forma a que este seja resiliente a possível alterações luminosas ou instabilidade de câmera.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; O melhoramento de imagem trata a imagem de forma a que a extração de componentes e propriedades contenham o mínimo de erros possíveis.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A extração de componentes conexos e propriedades determina os objetos presentes na imagem e trata individualmente todos estes objectos extraindo features que os caracterizam.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; A classificação determina a categoria a que cada objecto pertende, indentificado-o. E o tracking de objectos permite que mantenhamos informação sobre os objectos que permanecem no plano de imagem.</p>\n",
    "<p style=\"text-align: justify;text-align: justify;\">&emsp; Existem, no entanto, alguns apectos a serem melhorados, nomeadamente, zonas de convergência com outros objectos podem gerar uma classificação incorreta do objecto (exemplo: poste à frente do carro), zonas de próximidade entre objectos podem gerar uma classificação única quando deveriam ser classificados dois objectos distintos e zonas de limite da imagem, com a diminuição da área, podem fazer com que um carro, por momentos, seja identificado como sendo uma pessoa. Ainda assim, após o término do trabalho laboratorial proposto concluo que o objectivo geral foi cumprido sem dificuldades de maior. </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9892b01-2837-4a92-80e9-b09da1470bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42a5ee7-b604-41bd-93ea-ac2c924d9043",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
